<img src="../img/vighnesh-dudani-ZQSs0YZUNfA-unsplash.jpg" width=100% />
<div align=center><h4>Data Science Related Concepts</h4></div>

- [x] Data Types

    “Data is the new oil.” Today data is everywhere in every field. Whether you are a data scientist, marketer, businessman, data analyst, researcher, or you are in any other profession, you need to play or experiment with raw or structured data. This data is so important for us that it becomes important to handle and store it properly, without any error. While working on these data, it is important to know the types of data to process them and get the right results. There are two types of data: Qualitative and Quantitative data, which are further classified into four types of data: nominal, ordinal, discrete, and Continuous (Great Learning Team, 2021).
    
    *Qualitative or Categorical Data*
    - Qualitative or Categorical Data is data that can’t be measured or counted in the form of numbers. These types of data are sorted by category, not by number. That’s why it is also known as Categorical Data. These data consist of audio, images, symbols, or text. The gender of a person, i.e., male, female, or others, is qualitative data.
    - Qualitative data tells about the perception of people. This data helps market researchers understand the customers’ tastes and then design their ideas and strategies accordingly. 
    - Qualitative data can be classified into two parts.

    | Types of Data   | Description  |
    | --------------- | ------------ |
    | Nominal Data    | Nominal Data is used to label variables without any order or quantitative value. The colour of hair can be considered nominal data, as one colour can’t be compared with another colour |
    | Ordinal Data    | Ordinal data have natural ordering where a number is present in some kind of order by their position on the scale. These data are used for observation like customer satisfaction, happiness, etc., but we can’t do any arithmetical tasks on them |
    | Discrete Data   | The term discrete means distinct or separate. The discrete data contain the values that fall under integers or whole numbers. The total number of students in a class is an example of discrete data. These data can’t be broken into decimal or fraction values |
    | Continuous Data | Continuous data are in the form of fractional numbers. It can be the version of an android phone, the height of a person, the length of an object, etc. Continuous data represents information that can be divided into smaller levels. The continuous variable can take any value within a range.  |
    
    *Difference Between Nominal and Ordinal Data*
    
    | Nominal Data                                                                                  | Ordinal Data                                                                                                |
    | --------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
    | Nominal data can't be quantified, neither they have any intrinsic ordering                    | Ordinal data gives some kind of sequential order by their position on the scale                             |
    | Nominal data is qualitative data or categorical data                                          | Ordinal data is said to be "in-between" qualitative data and quantitative data                              |
    | They don't provide any quantitative value, neither we can perform any arithematical operation | They provide sequence and can assign numbers to oridinal data but cannot perform the arithmetical operation |
    | Nominal data cannot be used to compare with one another                                       | Ordinal data can help to compare one item with another by ranking or ordering                               |
    
    *Quantitative Data*
    - Quantitative data can be expressed in numerical values, which makes it countable and includes statistical data analysis. These kinds of data are also known as Numerical data. It answers the questions like, “how much,” “how many,” and “how often.” For example, the price of a phone, the computer’s ram, the height or weight of a person, etc., falls under the quantitative data. 
    - Quantitative data can be used for statistical manipulation and these data can be represented on a wide variety of graphs and charts such as bar graphs, histograms, scatter plots, boxplot, pie charts, line graphs, etc.
    
    *Difference Between Discrete and Continous Data*
    
    | Discrete Data | Continuous Data |
    | ------------- | --------------- |
    | Discrete data are countable and finite, they are whole numbers or integers | Continuous data are measureable, they are in the form of fraction or decimal |
    | Discrete data are represented mainly by bar graphs                         | Continuous data are represented in the form of a historgram                  |
    | The values cannot be divided into subdivisions into smaller pieces         | The values can be divided into subdivisions into smaller pieces              |
    | Discrete data have spaces between the values                               | Continuous data are in the form of a continuous sequence                     |
    
- [x] Data Formats (IBM Cloud Education)
    - Unstructured data
    
        In the modern world of big data, unstructured data is the most abundant. It’s so prolific because unstructured data could be anything: media, imaging, audio, sensor data, text data, and much more. Unstructured simply means that it is datasets (typical large collections of files) that aren’t stored in a structured database format. Unstructured data has an internal structure, but it’s not predefined through data models. It might be human generated, or machine generated in a textual or a non-textual format.
        
    - Semi-structured data
        
        Semi-structured data (e.g., JSON, CSV, XML) is the “bridge” between structured and unstructured data. It does not have a predefined data model and is more complex than structured data, yet easier to store than unstructured data.
         
    - Structured data
    
        Structured data — typically categorized as quantitative data — is highly organized and easily decipherable by machine learning algorithms. Developed by IBM in 1974, structured query language (SQL) is the programming language used to manage structured data. By using a relational (SQL) database, business users can quickly input, search and manipulate structured data.
    
    | Data Formats    | Pros | Cons |
    | --------------- | ---- | ---- |
    | Structured      | Easily used by machine learning (ML) algorithms<br>Easily used by business users<br>Accessible by more tools<br> | Limited usage<br>Limited storage options<br> |
    | Unstructured    | Native format<br>Fast accumulation rates<br>Data lake storage<br> | Requires expertise<br>Specialized tools |
    | Semi-structured | No constrainted to fixed architecture<br>More storable and portable<br>Flexible to schema | Hard to evaluate when scale is large |
        
- [ ] [Data Smoothing Methods]()

    In statistics and image processing, to smooth a data set is to create an approximating function that attempts to capture important patterns in the data, while leaving out noise or other fine-scale structures/rapid phenomena. In smoothing, the data points of a signal are modified so individual points higher than the adjacent points (presumably because of noise) are reduced, and points that are lower than the adjacent points are increased leading to a smoother signal. Smoothing may be used in two important ways that can aid in data analysis by being able to extract more information from the data as long as the assumption of smoothing is reasonable and by being able to provide analyses that are both flexible and robust. Many different algorithms are used in smoothing ([Wikipedia](https://en.wikipedia.org/wiki/Smoothing)).
    
    - [ ] [Simple Exponential]()
    - [ ] [Moving Average]()
    - [ ] [Random Walk]()
    - [ ] [Exponential Moving Average]()
    - [ ] [Laplace Smoothing]()
    - [ ] [Epsilon Smoothing]()
- [ ] [Data Wrangling]()
- [ ] Experimental Design
    - Future Selection
    - Dimensionality Reduction
    - Performance Evaluation
- [ ] Bias & Variance Tradeoff
- [ ] Uncertainty Quantification
- [ ] Differential Privacy
- [ ] Big Data Analytics (BDA)
    
    > The ability to collect, store, and process increasingly large and complex data sets from a variety of sources, into competitive advantage.
    
    - Stakeholders of BDA
        - Individual
        - Organizations
        - Society
        
- [ ] [Visual Assessment of Tendency (VAT) Plot]()
- [ ] [Parallel Coordinate Plot (PCP)]()
- [ ] [k-anonymity]()
- [ ] [l-diversity]()
- [x] [FAIR principle](https://www.go-fair.org/fair-principles/)
    
    FAIR Guiding Principles of scientific data management and stewardship provide guidelines to improve the **Findability, Accessibility, Interoperability, Reuse** of digital assest. The principle emphasis machine actionabiilty (i.e. the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention) because humans increasingly rely on computational support to dael with data as a result of the increase in volumne, complexity, and creation speed of data (GO FAIR).
    
    According to Go-Fair, it define the following **FAIRification process**.
    
    Findable:
    - (Meta)data are assigned a globally unique and persistent identifier
    - Data are described with rich metadata
    - Metadata clearly and explicityly include the identifier of the data they describe
    - (Meta)data are registered or indexed in a searchable resource
    
    Accessible:
    - (Meta)data are retrievable by their identifier using a standardised communications protocol
        - The protocol is open, free, and universally implementable
        - The protocol allows for an authentication and authorisation procedure, where necessary
    - Metadata are accessible, even when the data are no longer available
    
    Interoperable:
    - (Meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation
    - (Meta)data use vocabularies that follow FAIR principles
    - (Meta)data include qualified references to other (meta)data
    
    Reusable:
    - (Meta)data are richly described with a plurality of accurate and relevant attributes
        - (Meta)data are released with a clear and accessible data usage license
        - (Meta)data are associated with detailed provenance
        - (Meta)data meet domain-relevant community standards
    
    In short, follow the FAIR principle, a dataset will contain a meaningful metadata that describes where to find the data, how to access the data, how to use the data and finally help user optimise the reuse of data.
    
- [x] 5W1H

    5W1H stands for What? Who? Where? When? Why? How? This method consists of asking a systematic set of questions to collect all the data necessary to draw up a report of the existing situation with the aim to identifying the true nature of the problem and describing the context precisely (Humanperf Software, 2018).
    
    By asking the right questions, make the situation easy understand and problem-solving process rational and efficient (David, 2019).
    - What: description of the problem;
    - Who: the responsible parties;
    - Where: the location of the problem;
    - When: temporal characteristics of the problem (at what point in time, how often)
    - How: the effects of the problem?
    - Why: reasons, cause of the problems?

    In the real world, a situation may not have so many questions could be asked. However, it's a good practice to keep your mind focus on the situation or problem itself.
    
- [x] CI/CD
    > Isaac Sacolick: CI/CD is a best practice for devops and agile development. Here's how software development teams automate continuous integration and delivery all the way through the CI/CD pipeline.
    
    Continuous Integration (CI) is a coding philosophy and set of practices that derive development teams to frequently implement small code changes and check them into a version control repository. So the team could continuous intergrate and validate changes. Continuous integrations establishes an automated way to build, package and test their applications. This encourage the programmer commit more frequently which leads to better collaboartion and code code quality.
    
    Continouse Delivery (CD) picks up where continous intergration ends, and automates application delivery to selected environments, including production, development, and testing environments. Continuous delivery is an automated way to push code changes to these environments which allows the developer could continuous update small changes when CI is valid.
    
- [x] Scalability, Horizontal & Vertical Scaling

    Scalability is the property of a system to handle a growing amount of work by adding resources to the system (Wikipedia). It is a measure of a system's ability to increase or decrease in performance and cost in resopnse to changes in application and system processing demands.
    
    Scalability can be measured over multiple dimensions, such as:
    - Administrative scalability: The ability for an increasing number of organizations or users to access a system.
    - Functional scalability: The ability to enhance the system by adding new functionality without disrupting existing activities.
    - Geographic scalability: The ability to maintain effectiveness during expansion from a local area to a larger region.
    - Load scalability: The ability for a distributed system to expand and contract to accommodate heavier or lighter loads, including, the ease with which a system or component can be modified, added, or removed, to accommodate changing loads.
    - Generation scalability: The ability of a system to scale by adopting new generations of components.
    - Heterogeneous scalability is the ability to adopt components from different vendors.

    Most of time, people may talk scale a system horizontally or vertially:
    
    - Horizontal Scaling
        
        Horizontal scaling refers to adding addtional nodes or machines to respond new demands (CloudZero, 2021). For example, if a web server has a increase demand on network traffic, by horizontal scaling, we add more server to increase the access nodes for future users.
        
        | Advantages                                    | Disadvantages                                     |
        | --------------------------------------------- | ------------------------------------------------- |
        | Scaling is easier from a hardware perspective | Increased complexity of maintenance and operation |
        | Fewer periods of downtime                     | Increased inital costs                            |
        | Increase resilience and fault tolerance       |                                                   |
        | Increaseed performance                        |                                                   |
        
    - Vertical Scaling
        
        Vertical scaling refers to distribute more resources or add more power to the current machine (CloudZero, 2021). For example, by upgrading CPUs, increase RAM size to increase the server computing power.
        
        | Advantages                         | Disadvantages                   |
        | ---------------------------------- | ------------------------------- |
        | Cost-effective                     | Higher possibility for downtime | 
        | Less complex process communication | Single point of failure         |
        | Less complicated maintainance      | Upgrade limitation              |
        | Less need for software changes     |                                 |
    
    Depends on the demands, you may choose horizontal or vertical scaling based on factors like: cost, future-proofing, topographic distribution, reliability, upgradeability and flexibility, or performance and complexity.
    
- [x] Customer Relationship Management (CRM)

    Customer relationship management (CRM) is a technology for managing all your company’s relationships and interactions with customers and potential customers. The goal is simple: Improve business relationships to grow your business. A CRM system helps companies stay connected to customers, streamline processes, and improve profitability.

    When people talk about CRM, they are usually referring to a CRM system, a tool that helps with contact management, sales management, agent productivity, and more. CRM tools can now be used to manage customer relationships across the entire customer lifecycle, spanning marketing, sales, digital commerce, and customer service interactions.

    A CRM solution helps you focus on your organization’s relationships with individual people — including customers, service users, colleagues, or suppliers — throughout your lifecycle with them, including finding new customers, winning their business, and providing support and additional services throughout the relationship.
    
    More information please check [here](https://www.salesforce.com/crm/what-is-crm/).
    
- [ ] Game Theory

- [ ] SWOT Analysis
    - Strength
    - Weakness
    - Opportunities
    - Threats
    
- [x] [Zook's 10 Rules For Responisble Big Data Research](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005399)

    - Acknowledge that data are people and can do harm.
    - Recognize that privacy is more than a binary value.
    - Guard against the reidentification of your data.
    - Practice ethical data sharing.
    - Consider the strengths and limitations of your data; big does not automatically mean better.
    - Debate the tough, ethical choices.
    - Develop a code of conduct for your organization, research community, or industry.
    - Design your data and systems for auditability.
    - Engage with the broader consequences of data and analysis practices.
    - Know when to break these rules.
    
- [x] SMART Goals
    
    SMART goals stands for Specific, Measurable, Achievable, Relevant, and Time-Bound (Kat, 2021).
    
    Specific: You need to have a specific goal for effectiveness, in general you could ask question like:
    - What needs to be accomplished?
    - Who's responsible for it?  
    - What steps need to be taken to achieve it?
    
    > e.g. Grow the number of monthly users of Techfirm’s mobile app by optimizing our app-store listing and creating targeted social media campaigns.
    
    Measurable: Goals must be measurable, set milestones to check your working progress. Or setting a trackable benchmark.
    
    > Increase the number of monthly users of Techfirm’s mobile app by 1,000 by optimizing our app-store listing and creating targeted social media campaigns for four social media platforms: Facebook, Twitter, Instagram, and LinkedIn.
    
    Achievable: Goals must be realistic, realistic goals brings true outcomes!
    
    > Increase the number of monthly users of Techfirm’s mobile app by 1,000 by optimizing our app-store listing and creating targeted social media campaigns for three social media platforms: Facebook, Twitter, and Instagram.
    
    
    Relevant: Current goals must align with the project targets, think about the big picture, ask question: Why are you setting the goal that you're setting?
    
    > Grow the number of monthly users of Techfirm’s mobile app by 1,000 by optimizing our app-store listing and creating targeted social media campaigns for three social media platforms: Facebook, Twitter, and Instagram. Because mobile users tend to use our product longer, growing our app usage will ultimately increase profitability.
    
    Time-Bounded: To properly measure success, you and your team need to be on the same page about when a goal has been reached. Find a precise time-bound could help you track with a designed time framework.
    
    > Grow the number of monthly users of Techfirm’s mobile app by 1,000 within Q1 of 2022. This will be accomplished by optimizing our app-store listing and creating targeted social media campaigns, which will begin running in February 2022, on three social media platforms: Facebook, Twitter, and Instagram. Since mobile is our primary point of conversion for paid-customer signups, growing our app usage will ultimately increase sales.
    
- [x] OSEMN Framework
    
    OSEMN stands for Obtain, Scrub, Explore, Model, and iNterpret.
    
    <img src="https://miro.medium.com/max/1400/1*eE8DP4biqtaIK3aIy1S2zA.png" alt="Data Science Process (a.k.a the O.S.E.M.N. framework)" width=100%>
    
    **Obtain**: When the project start, we need to obtain data from available data sources. For example query from database, or using web scrapping techniques.
    
    **Scurb**: Clean the data and convert data into a canonical format for future task. For example, you may need to eliminate outliers or missing values and standardising data into a uniform format.
    
    **Explore**: Check dataset insights to find meaningful feature for future machine learning work.
    
    **Model**: Construct models based on explored features.
    
    **iNterpret**: Explain the model by checking the ability for unseen data, or using visualisation to cheek its performance.
    
- [x] Team Data Science Process Framework (TDSP)
    
    The Team Data Science Process (TDSP) is an agile, iterative data science methodology to deliver predictive analytics solutions and intelligent applications efficiently. TDSP helps improve team collaboration and learning by suggesting how team roles work best together. TDSP includes best practices and structures from Microsoft and other industry leaders to help toward successful implementation of data science initiatives. The goal is to help companies fully realize the benefits of their analytics program.
    
    Detailed description is available on [Microsoft Doc](https://docs.microsoft.com/en-us/azure/architecture/data-science-process/overview).
    
    The framework main focus on the lifecycle outlines the major stages that projects typically execute, often iteratively:
    - Business understanding
    - Data acquisition and understanding
    - Modeling
    - Deployment
    
    <img src="https://docs.microsoft.com/en-us/azure/architecture/data-science-process/media/overview/tdsp-lifecycle2.png" witdh=100%>
    
    It provides a standard workflow for working on a big data analysis project or construct a data oriented product. Microsoft also provides a [template repository](https://github.com/Azure/Azure-TDSP-ProjectTemplate) for project initiate.
    
- [x] Cross-Industry Standard Process for Data Mining Framework (CRISP-DM)

    From [Wikipedia](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining): Cross-industry standard process for data mining, known as CRISP-DM,[1] is an open standard process model that describes common approaches used by data mining experts. It is the most widely-used analytics model.[2]

    In 2015, IBM released a new methodology called Analytics Solutions Unified Method for Data Mining/Predictive Analytics[3][4] (also known as ASUM-DM) which refines and extends CRISP-DM.
    
    It go through six main phases:
    - Business understanding: understand business content and requirements
    - Data understanding: understand the data based on meta data or provided data dictionary
    - Data preparation: processing data in a nice and clean format
    - Modeling: construct data product
    - Evaluation: check data product performance
    - Deployment: publish the product

    The sequence of the phases is not strict and moving back and forth between different phases is usually required. The arrows in the process diagram indicate the most important and frequent dependencies between phases. The outer circle in the diagram symbolizes the cyclic nature of data mining itself. A data mining process continues after a solution has been deployed. The lessons learned during the process can trigger new, often more focused business questions, and subsequent data mining processes will benefit from the experiences of previous ones.